'''
Created on Jul 19, 2016

@author: Lee J
'''
#from lxml import html
#import requests
#from lxml.etree import XPath
#import re
#from _cffi_backend import string
from bs4 import BeautifulSoup
from urllib2 import urlopen
from conda_build import create_test


if __name__ == '__main__':
    print "hello world"
    
    url = 'http://www.babynameguide.com/'
    primary_addendum = 'categories.html'
    
    '''define the function to get all the links for scraping the site'''
    def get_category_links(section_url):
        html = urlopen(section_url).read()
        soup = BeautifulSoup(html, "lxml")
        cats = soup.find("table", "NameCategories")
        category_links = [url + li.a["href"] for li in cats.findAll("li")]
        #print category_links
        return category_links
    
    category_links = get_category_links(url+primary_addendum)
    letter_links = []
    alphabet = "abcdefghijklmnopqrstuvwxyz"
    alphabet.capitalize()
    print alphabet
    
    print "Category links: ", category_links
    
    def get_letter_link(category_link):
        html = urlopen(category_link).read()
        soup = BeautifulSoup(html, "lxml")
        cats = soup.find("table", "NameCategories")
        letter_link = [url + tr.a["href"] for tr in cats.findAll("tr", limit=1)]
        return letter_link
    
    iterthing = 0
    
    
        
       
    for category_link in category_links:
        letter_link = get_letter_link(category_link)
        print "Letter links: ", letter_link
        #add letter link to letter links list for later use
        letter_links.append(letter_link[0])
        
        
        ##cut up string so we can replace the letter with something from the alphabet
        
        
        ###
        #Also get all names from that page and write them to the file
        ###
        if iterthing == 2:
            break #for test only.  Will comment before live go
        else:
            iterthing += 1
    
    print letter_links
    
    
    for link in letter_links:
        for letter in alphabet:
            break
    '''
    #ONE WAY OF SCRAPING
    page = requests.get(url)
    
    this_page = page.content
    print this_page
    
    
    pattern = re.compile('<li><a href=.[*]</li>', re.IGNORECASE)
    m = re.findall(pattern, this_page)
    if m:
        print 'Match:', m
    else:
        print 'No match'
    '''
    '''
    f = open('page_info.txt','w') #opens file with this name
    
    f.write(result)
    f.close()
    '''
    #print this_page
    
    
    
    '''
    page = requests.get(url+primary_addendum)
    tree = html.fromstring(page.content)
    
    categories = tree.xpath('//a[@href="category*"]')
    
    print categories
    '''
    '''
    f = open('page_info.txt','w') #opens file with this name
    
    f.write(categories)
    f.close()
    '''
    #how can we find the data using XPATH?
    
    #get strings for each category and name it
    
    


